---
title: "Report on a pre-test of the TVJ-task"
author: "Michael"
date: "3/17/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
d = read_csv('../../data/02_TVJ_pretest/data_raw.csv') %>% 
  mutate(response = ifelse(response == "TRUE", 1, 0))
d = d[-which(d$comments == "Test run by MF"),] # exclude test run by MF
```

20 participants were drafted on Prolific to complete a truth-value judgement task. Each participant was paid 1.20 pounds for compensation. The average time spent on the whole experiment (including reading instructions, and completing the post-experiment survey) was `r d$timeSpent %>% mean %>% round(2)` minutes.

The experiment can be examined [here](https://michael-franke.github.io/misleading_with_underinfo/experiments/02_TVJ_pretest/index.html). The task consisted of 60 sentences presented completely at random from the following data set:

```{r, echo = F}
select(d, condition, trigger, question) %>% arrange(condition, trigger, question) %>% unique() %>% print.data.frame()
```

Let's first check whether anyone left any important comments, which appears to be not the case:

```{r}
d %>% pull(comments) %>% unique()
```

The mean rate of "TRUE" judgements for each condition is:

```{r}
d_summary = d %>% group_by(trigger, condition, question) %>% 
  summarize(mean_true = mean(response)) %>% arrange(condition)
d_summary %>% print.data.frame
```

Averaging over sentence material for each trigger-condition pair we get the following mean TRUE judgement ratings:

```{r}
d_summary %>% group_by(condition, trigger) %>% 
  summarize(mean_true = mean(mean_true)) %>% 
  ggplot(aes(x = trigger, y = mean_true)) + 
  geom_bar(stat = "identity", position = "dodge") +
  facet_grid(~condition)
```

We see that the 'bizarre' control sentences are rated as false (usually) and that the 'true' controls receive rather high rates of 'true' judgements. However, the latter are not at ceiling and, most importantly: **the amount of 'true' endorsements for /most/ and /some/ in the implicatures condition is very high**. In contrast, as expected, sentences with /number/ words in the implicature condition are judged false. (But notice that these also have a slightly different sentences structure, perhaps suggesting a different information structure; we should not make a big deal of comparing /number/ on the one hand to /most/ & /some/ on the other hand in the implicature condition.)

```{r}
d_summary_id = d %>% group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% arrange(submission_id) %>% 
  filter(condition == "implicature") 
```

Finally, it is also interesting to see how individual participants behaved when confronted with implicature-items. The following looks at the mean 'true' judgements for /some/ and /most/ sentences in the implicature condition, for each participant individual. The histogram shows the distribution of each participant's mean acceptance rate of the implicature sentences (excluding /number/ because everybody judged these 'false' anyway.)

```{r}
d %>% group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% arrange(submission_id) %>% 
  filter(condition == "implicature", trigger != "number") %>% 
  group_by(submission_id) %>% 
  summarize(mean_true = mean(mean_true)) %>% 
  ggplot(aes(x = mean_true)) + geom_histogram(bins = 22)
```


We see that 9 out of 20 participants endorsed all occurrences of sentences with a false scalar implicature as 'true'. 15 out of 20 participants endorsed them as 'true' with a rate of at least 7 out of 10. (Remember that each participant saw 10 items in the implicature condition, 5 with /some/ and 5 with /most/.)

If we split this up further into the mean judgements for /some/ vs /most/, the picture is:

```{r}
d %>% group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% arrange(submission_id) %>% 
  filter(condition == "implicature", trigger != "number") %>% 
  group_by(submission_id, trigger) %>% 
  summarize(mean_true = mean(mean_true)) %>% 
  ggplot(aes(x = mean_true)) + geom_histogram(bins = 11) +
  facet_grid(.~trigger)
```

This suggests that /some/ might be more useful for dissecting participants into semantic and pragmatic responders: 6 out of 19 participants rather consistently judged the sentences with a false scalar implicature as 'false'. That is still a rather low rate, though.

Finally, taking together the individual averages of 'true' judgements for /most/ and /some/, the 2D space in which participants reside with their answer profile looks like this:

```{r}
d %>% group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% arrange(submission_id) %>% 
  filter(condition == "implicature", trigger != "number") %>% 
  group_by(submission_id, trigger) %>% 
  summarize(mean_true = mean(mean_true)) %>% 
  spread(key = trigger, value = mean_true)  %>% 
  ggplot(aes(x = some, y = most)) + 
  geom_point()
```

Although this data set is way too sparse, speculating wildly, it may appear as if there are three types of respondents:

1. those who judge /some/ and /most/ implicature-triggering sentences true
2. those who judge /most/ implicature-triggering sentences true, but /some/-sentences false
3. those who judge /some/ and /most/ implicature-triggering sentences false