---
title: "Report on final run of complete experiment"
author: "Michael"
date: "02/5/2019"
output: html_document
---

```{r setup, include=FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cowplot)
library(magrittr)
d = read_csv('../../data/06_sentence_select_with_training/data_raw_final.csv') %>% 
  filter(! prolific_id %in% c("TEST_MF")) %>% 
  filter(submission_id >= 7688)
```

## Data collection and design

`r d$submission_id %>% unique() %>% length` participants were drafted on Prolific to do the complete experiment. On each start of the experiment, participants where randomly assigned to one of three conditions:

1. cooperative play with a literal co-player
2. competitive play with a level-0 (semantic/unstrategic) co-player
3. competitive play with a level-1 (anti-semantic/strategic) co-player

The experiment consisted of (in order) the following parts:

1. instructions
2. color blindness test
3. 4 training trials in the role as guesser
4. 18 training trials in describer role with feedback
    - the feedback was auto-generated and corresponded to a literal interpreting agent ('cooperative' and 'unstrategic condition') or an agent who inverts the semantic meaning of descriptions ('strategic condition')
    - this training round used the /all/ and /none/ data from the main trial (reused exact same pictures)
5. 36 trials of the sentence completion w/o feedback (main task)
6. self-assessment of performance in main task
7. truth-value judgement task
    - participants were asked to rate how they believed the co-player interpreted sentences
    
Payment was 2.5 pounds for compensation plus 0.75 pounds bonus. The bonus was announced to be dependent on performance in the card-playing task, but evenutally paid to everyone. The average time spent on the whole experiment (including reading instructions, and completing the post-experiment survey) was `r d$timeSpent %>% mean %>% round(2)` minutes.

The experiment can be examined [here](https://michael-franke.github.io/misleading_with_underinfo/experiments/06_sentence_select_with_training/index.html).

The motivation for an initial training-with-feedback round is that the training trials should let us infer (roughly) which theory each participant might have had about the co-player's behavior. In particular, we will be interested in the behavior of participants who learned to predict the co-player's choices during training. We will also use the data from the TVJ task to then classify participants into four types along two binary classifications, namely (i) those that expect the co-player to interpret semantically vs. pragmatically and (ii) those that expect the co-player to interpret strategically (non-literally) vs unstrategically (literally).

Our research questions are (NB: not really hypotheses but rather "questions about nature"):

1. Do people understand the cooperative task, as demonstrated by good overall perfomance? (base-line, control)
2. Will they use a putative implicature enrichment to strategically mislead the opponent?

The second question can really only be answered based on the choice data from the main task, if we know more about the participants' likely beliefs about their opponent. For that reason, we take the types described above into account. We expect to be able to see a strategic use of implicature meaning only for those participants who believe that their opponent computes implicatures and who understood whether their opponent is literal or strategic (level-0 vs level-1).

## Exploring the data

Here's what participants commented:

```{r}
d$comments %>% unique
```

Here's the outcome of the (randomized) allocation to 'strategic' vs. 'unstrategic' co-players (training set).

```{r}
show(table(d$coplayer_type)/122)
```

## Data preparation

We looked at the behavior of each participant in the training & TVJ tasks. We want to classify each participant based on successful/unsuccessful training and a semantic/pragmatic type.

### Color blindness

We first look at answers to the colorblindness test and add that to our data set:

```{r}
color_blindness = filter(d, trial_type == "color_blindness_test") %>%
  select(submission_id, correct, response) %>%
  mutate(correct = ifelse(correct == response,1,0)) %>%
  group_by(submission_id) %>%
  summarize(color_blind = mean(correct) < 1)
color_blindness_failures = filter(color_blindness, color_blind == TRUE) %>% pull(submission_id)
message("Number of failures on color blindness test: ", length(color_blindness_failures))
## exclude color blind participants right away?
# d = filter(d, ! submission_id %in% color_blindness_failures)

# add info to full data set
d = full_join(d, color_blindness, by = "submission_id")
```

Here's the kind of answers that people who "failed"" the colorblindness test gave:

```{r}
d %>% filter(color_blind == TRUE, trial_type == "color_blindness_test") %>% 
  unique() %>% select(submission_id, correct, response) %>% 
  spread(key = correct, value = response)
```

We see that nobody appears to be completely color blind. It might be justify not to exclude anybody based on misperceiving 73 as 13, 23 or 78. At least, nobody wrote "none", which as the response they were instructed to give if they did not perceive any number.

### Training phase

To classify successful/unsuccessful training, we note that there is a "winning move" for the (dummy) co-player. For an unstrategic co-player, participants should choose option /green/ to win with certainty; for a strategic co-player, it should be /red/. We classify a participant's training as successful iff they achieve an average number of "winning move" choices of at least `success_threshold` (defined below). 

```{r}
success_threshold = 0.4

d_training = filter(d, trial_type == "sentence_completion_training")

d_training_summary = d_training %>% 
  mutate(winning_move = ifelse(coplayer_type == "unstrategic", "red", "green"),
         winning_move_chosen = ifelse(winning_move == response, 1, 0),
         loosing_move = ifelse(coplayer_type == "unstrategic", "green", "red"),
         loosing_move_chosen = ifelse(loosing_move == response, 1, 0),
         distractor_move_chosen = ifelse(response == "false", 1, 0)) %>% 
  group_by(submission_id, coplayer_type, condition) %>% 
  summarize(win = mean(winning_move_chosen),
            loose = mean(loosing_move_chosen),
            dstrct = mean(distractor_move_chosen)) %>% 
  ungroup() %>% 
  gather(key = response, value = proportion, win, loose, dstrct) %>% 
  mutate(new_col_names = paste0("train_", condition, "_", response)) %>% 
  select(submission_id, new_col_names, proportion) %>% 
  spread(key = new_col_names, value = proportion) %>% 
  group_by(submission_id) %>% 
  mutate(training_successful = ifelse(mean(c(train_all_win, train_none_win)) > success_threshold, 1, 0)) %>% 
  ungroup()

d = full_join(d, d_training_summary, by = "submission_id")
```

Let's look at how many people learned their opponent's type during training:

```{r}
message("Nr learned from training?")
d %>% select(submission_id, coplayer_type, training_successful) %>% 
  unique() %>% 
  group_by(coplayer_type, training_successful) %>% 
  summarize(count = n()) %>% show()

```

It seems that learning is easiest in the cooperative condition, and hardest in the competitive condition with an unstrategic opponent, but that is only by inspection and not particularly relevant.

### TVJ 

To classify participants as (expecting) semantic or pragmatic (co-player interpretations), we look at the overal proportion of semantic (/true/) responses to implicature items /some/ and /most/. If this exceeds the threshold `semprag_threshol`, participants are classified as semantic responders. We also check participants behavior on the control items, and consider a participant ready for exclusion if their performance in the controls is below the `control_threshold`.

```{r}
control_threshold = 0.7
semprag_threshold = 0.5

d_tvj = filter(d, trial_type == "truth_value_judgements") %>% 
  mutate(response = ifelse(response == "true", 1, 0))

d_tvj_summary = d_tvj %>% 
  group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% 
  arrange(submission_id) %>% 
  ungroup() %>% 
  mutate(new_col_names = paste0("tvj_", trigger, "_", condition)) %>% 
  select(submission_id, new_col_names, mean_true) %>% 
  spread(key = "new_col_names", value = mean_true) 

d_tvj_controls = d_tvj %>% 
  filter( condition != "implicature" ) %>% 
  group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% 
  arrange(submission_id) %>% 
  ungroup() %>% 
  mutate(control_score = ifelse(condition == "true", mean_true, 1-mean_true)) %>% 
  group_by(submission_id) %>% 
  summarize(tvj_control_mean = mean(control_score),
            tvj_control_fail = ifelse(tvj_control_mean < control_threshold, 1, 0))

message("Number of people who performed really badly in the TVJ: ", 
        d_tvj_controls$tvj_control_fail %>% sum())

d_tvj_implicatures = d_tvj %>% 
  filter( condition == "implicature", trigger != "number" ) %>% 
  group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% 
  group_by(submission_id) %>% 
  summarize(tvj_implicature_mean = mean(mean_true),
            tvj_semprag_type = ifelse(tvj_implicature_mean <= semprag_threshold, 
                                      "pragmatic", "semantic"))

message("Number of participants who expected co-player to interpret pragmatically in TVJ: ", 
        sum(d_tvj_implicatures$tvj_semprag_type == "pragmatic"))

d = full_join(d, d_tvj_summary, by = "submission_id")
d = full_join(d, d_tvj_controls, by = "submission_id")
d = full_join(d, d_tvj_implicatures, by = "submission_id")

```

When then exclude the bad performers from the TVJ-task:

```{r}
# exclude bad performers
d = filter(d, tvj_control_fail == 0)
```

### How many different types do we have?

Here are the counts from this cross-classification:

```{r}
d %>% filter(tvj_control_fail == 0) %>% 
  select(submission_id, 
         coplayer_type, 
         training_successful, 
         tvj_semprag_type, 
         tvj_control_fail) %>% 
  unique() %>% 
  group_by(coplayer_type, training_successful, tvj_semprag_type) %>% 
  summarize(count = n()) %>% show()
```

Though this is not important for our puproses, it may seem that pragmatic players are more likely to pass training.

## Data from main part

We will plot choice data from the main trials based on the 2x2 classifications we made based on the data from other parts of the experiment. **Notice that the normative "winning move" of participants with a "strategic" training regime is to play /green/; that for the "unstrategic" condition /red/.**

Here's a convenience plotting function:

```{r}
get_group_plot = function(sp_type = "pragmatic", success = 1) {
  group_data = filter(d, 
                      trial_type == "sentence_completion", 
                      # coplayer_type != "cooperative",
                      # color_blind == FALSE,
                      tvj_semprag_type == sp_type, 
                      training_successful == success) %>% 
    mutate(condition = factor(condition, ordered = T, 
                              levels = c("all", "none", "number", "ad_hoc", "some"))) 
  N = group_data$submission_id %>% unique() %>% length()
  group_data %>% ggplot(aes(x = response)) + 
    geom_bar(aes( y = ..prop.., group = 1)) +
    coord_flip() +
    facet_grid(coplayer_type ~ condition, scales = "free") +
    ggtitle(paste0(sp_type, " responders with ", 
                   ifelse(success, "successful", "unsuccessful"), 
                   " training (N=", N, ")"))
}
```

And here is a combined plot

```{r, fig.height = 10, fig.width = 12}
prag_success = get_group_plot("pragmatic", 1)
prag_fail = get_group_plot("pragmatic", 0)
sem_success = get_group_plot("semantic", 1)
sem_fail = get_group_plot("semantic", 0)

plot_grid(
  prag_success,
  prag_fail,
  sem_success,
  sem_fail,
  labels = c("A", "B", "C", "D"), 
  align = "h",
  ncol = 2) %>% show()
```

