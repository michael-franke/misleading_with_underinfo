---
title: "Report on final run of complete experiment"
author: "Michael"
date: "02/5/2019"
output: html_document
---

```{r setup, include=FALSE, message = FALSE, warning=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cowplot)
library(magrittr)
library(nnet)
library(brms)
options (mc.cores=parallel::detectCores ()) # Run on multiple cores
d = read_csv('../../data/06_sentence_select_with_training/data_raw_final.csv') %>% 
  filter(! prolific_id %in% c("TEST_MF")) %>% 
  filter(submission_id >= 7688)
```

## Data collection and design

`r d$submission_id %>% unique() %>% length` participants () were drafted on Prolific to do the complete experiment. On each start of the experiment, participants where randomly assigned to one of three conditions:

1. cooperative play with a literal co-player
2. competitive play with a level-0 (semantic/unstrategic) co-player
3. competitive play with a level-1 (anti-semantic/strategic) co-player

The experiment consisted of (in order) the following parts:

1. instructions
2. color blindness test
3. 4 training trials in the role as guesser
4. 18 training trials in describer role with feedback
    - the feedback was auto-generated and corresponded to a literal interpreting agent ('cooperative' and 'unstrategic condition') or an agent who inverts the semantic meaning of descriptions ('strategic condition')
    - this training round used the /all/ and /none/ data from the main trial (reused exact same pictures)
5. 36 trials of the sentence completion w/o feedback (main task)
6. self-assessment of performance in main task
7. truth-value judgement task
    - participants were asked to rate how they believed the co-player interpreted sentences
    
Payment was 2.5 pounds for compensation plus 0.75 pounds bonus. The bonus was announced to be dependent on performance in the card-playing task, but evenutally paid to everyone. The average time spent on the whole experiment (including reading instructions, and completing the post-experiment survey) was `r d$timeSpent %>% mean %>% round(2)` minutes.

The experiment can be examined [here](https://michael-franke.github.io/misleading_with_underinfo/experiments/06_sentence_select_with_training/index.html).

The motivation for an initial training-with-feedback round is to induce in the participants a particular belief about the behavior of the co-player. This is to reduce degrees of freedom in the eventual interpretation of participants behavior in the main task. Consequently, we will be interested chiefly in the behavior of participants who learned to predict the co-player's choices during training. Indeed, we will exclude from the analysis participants who did not perform well enough during training, since for these participants it is hard to say which choices count as rational or random. We will also use the data from the TVJ task to further classify participants based on their beliefs about the co-player's interpretation of sentences involving potential implicature triggers. In total four types of participants will be distinguished along two binary classifications, namely (i) those that expect the co-player to interpret semantically vs. pragmatically and (ii) those that expect the co-player to interpret strategically (non-literally) vs unstrategically (literally).

Focusing on the participants who mastered the training (learned about the co-player's behavior), we would like to know the extend to which participants play the "winning move" of the respective experimental condition. The **winning move** of each condition is the move which, given the co-player behavior displayed during training, would lead to winning the round. In implicature trials, the "winning move" includes assuming that the co-player calculates the implicature. (So, with a strategic co-player in the competitive condition, the winning move assumes that the co-player calculates the implicature and therefore chooses the opposite card, of which the implicature meaning is not true). 

We can then formulate our research questions as follows. For the participants who mastered training:

1. Do 
2. Will they use a putative implicature enrichment to strategically mislead the opponent?

The second question can really only be answered based on the choice data from the main task, if we know more about the participants' likely beliefs about their opponent. For that reason, we take the types described above into account. We expect to be able to see a strategic use of implicature meaning only for those participants who believe that their opponent computes implicatures and who understood whether their opponent is literal or strategic (level-0 vs level-1).

## Exploring the data

Here's what participants commented:

```{r}
d$comments %>% unique
```

Here's the outcome of the (randomized) allocation to 'strategic' vs. 'unstrategic' co-players (training set).

```{r}
show(table(d$coplayer_type)/122)
```

## Data preparation

We looked at the behavior of each participant in the training & TVJ tasks. We want to classify each participant based on successful/unsuccessful training and a semantic/pragmatic type.

### Color blindness

We first look at answers to the colorblindness test and add that to our data set:

```{r}
color_blindness = filter(d, trial_type == "color_blindness_test") %>%
  select(submission_id, correct, response) %>%
  mutate(correct = ifelse(correct == response,1,0)) %>%
  group_by(submission_id) %>%
  summarize(color_blind = mean(correct) < 1)
color_blindness_failures = filter(color_blindness, color_blind == TRUE) %>% pull(submission_id)
message("Number of failures on color blindness test: ", length(color_blindness_failures))
## exclude color blind participants right away?
# d = filter(d, ! submission_id %in% color_blindness_failures)

# add info to full data set
d = full_join(d, color_blindness, by = "submission_id")
```

Here's the kind of answers that people who "failed"" the colorblindness test gave:

```{r}
d %>% filter(color_blind == TRUE, trial_type == "color_blindness_test") %>% 
  unique() %>% select(submission_id, correct, response) %>% 
  spread(key = correct, value = response)
```

We see that nobody appears to be completely color blind. It might be justify not to exclude anybody based on misperceiving 73 as 13, 23 or 78. At least, nobody wrote "none", which as the response they were instructed to give if they did not perceive any number.

### Training phase

To classify successful/unsuccessful training, we note that there is a "winning move" for the (dummy) co-player. For an unstrategic co-player, participants should choose option /green/ to win with certainty; for a strategic co-player, it should be /red/. We classify a participant's training as successful iff they achieve an average number of "winning move" choices of at least `success_threshold` (defined below). 

```{r}
success_threshold = 0.4

d_training = filter(d, trial_type == "sentence_completion_training")

d_training_summary = d_training %>% 
  mutate(winning_move = ifelse(coplayer_type == "unstrategic", "red", "green"),
         winning_move_chosen = ifelse(winning_move == response, 1, 0),
         loosing_move = ifelse(coplayer_type == "unstrategic", "green", "red"),
         loosing_move_chosen = ifelse(loosing_move == response, 1, 0),
         distractor_move_chosen = ifelse(response == "false", 1, 0)) %>% 
  group_by(submission_id, coplayer_type, condition) %>% 
  summarize(win = mean(winning_move_chosen),
            loose = mean(loosing_move_chosen),
            dstrct = mean(distractor_move_chosen)) %>% 
  ungroup() %>% 
  gather(key = response, value = proportion, win, loose, dstrct) %>% 
  mutate(new_col_names = paste0("train_", condition, "_", response)) %>% 
  select(submission_id, new_col_names, proportion) %>% 
  spread(key = new_col_names, value = proportion) %>% 
  group_by(submission_id) %>% 
  mutate(training_successful = ifelse(mean(c(train_all_win, train_none_win)) > success_threshold, 1, 0)) %>% 
  ungroup()

d = full_join(d, d_training_summary, by = "submission_id")
```

Let's look at how many people learned their opponent's type during training:

```{r}
message("Nr learned from training?")
d %>% select(submission_id, coplayer_type, training_successful) %>% 
  unique() %>% 
  group_by(coplayer_type, training_successful) %>% 
  summarize(count = n()) %>% show()

```

It seems that learning is easiest in the cooperative condition, and hardest in the competitive condition with an unstrategic opponent, but that is only by inspection and not particularly relevant.

### TVJ 

To classify participants as (expecting) semantic or pragmatic (co-player interpretations), we look at the overal proportion of semantic (/true/) responses to implicature items /some/ and /most/. If this exceeds the threshold `semprag_threshol`, participants are classified as semantic responders. We also check participants behavior on the control items, and consider a participant ready for exclusion if their performance in the controls is below the `control_threshold`.

```{r}
control_threshold = 0.7
semprag_threshold = 0.5

d_tvj = filter(d, trial_type == "truth_value_judgements") %>% 
  mutate(response = ifelse(response == "true", 1, 0))

d_tvj_summary = d_tvj %>% 
  group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% 
  arrange(submission_id) %>% 
  ungroup() %>% 
  mutate(new_col_names = paste0("tvj_", trigger, "_", condition)) %>% 
  select(submission_id, new_col_names, mean_true) %>% 
  spread(key = "new_col_names", value = mean_true) 

d_tvj_controls = d_tvj %>% 
  filter( condition != "implicature" ) %>% 
  group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% 
  arrange(submission_id) %>% 
  ungroup() %>% 
  mutate(control_score = ifelse(condition == "true", mean_true, 1-mean_true)) %>% 
  group_by(submission_id) %>% 
  summarize(tvj_control_mean = mean(control_score),
            tvj_control_fail = ifelse(tvj_control_mean < control_threshold, 1, 0))

message("Number of people who performed really badly in the TVJ: ", 
        d_tvj_controls$tvj_control_fail %>% sum())

d_tvj_implicatures = d_tvj %>% 
  filter( condition == "implicature", trigger != "number" ) %>% 
  group_by(submission_id, trigger, condition) %>% 
  summarize(mean_true = mean(response)) %>% 
  group_by(submission_id) %>% 
  summarize(tvj_implicature_mean = mean(mean_true),
            tvj_semprag_type = ifelse(tvj_implicature_mean <= semprag_threshold, 
                                      "pragmatic", "semantic"))

message("Number of participants who expected co-player to interpret pragmatically in TVJ: ", 
        sum(d_tvj_implicatures$tvj_semprag_type == "pragmatic"))

d = full_join(d, d_tvj_summary, by = "submission_id")
d = full_join(d, d_tvj_controls, by = "submission_id")
d = full_join(d, d_tvj_implicatures, by = "submission_id")

```

When then exclude the bad performers from the TVJ-task:

```{r}
# exclude bad performers
d = filter(d, tvj_control_fail == 0)
```

### How many different types do we have?

Here are the counts from this cross-classification:

```{r}
d %>% filter(tvj_control_fail == 0) %>% 
  select(submission_id, 
         coplayer_type, 
         training_successful, 
         tvj_semprag_type, 
         tvj_control_fail) %>% 
  unique() %>% 
  group_by(coplayer_type, training_successful, tvj_semprag_type) %>% 
  summarize(count = n()) %>% show()
```

Though this is not important for our puproses, it may seem that pragmatic players are more likely to pass training.

## Data from main part

We will plot choice data from the main trials based on the 2x2 classifications we made based on the data from other parts of the experiment. **Notice that the normative "winning move" of participants with a "strategic" training regime is to play /green/; that for the "unstrategic" condition /red/.**

Here's a convenience plotting function:

```{r}
get_group_plot = function(sp_type = "pragmatic", success = 1) {
  group_data = filter(d, 
                      trial_type == "sentence_completion", 
                      # coplayer_type != "cooperative",
                      # color_blind == FALSE,
                      tvj_semprag_type == sp_type, 
                      training_successful == success) %>% 
    mutate(condition = factor(condition, ordered = T, 
                              levels = c("all", "none", "number", "ad_hoc", "some"))) 
  N = group_data$submission_id %>% unique() %>% length()
  group_data %>% ggplot(aes(x = response)) + 
    geom_bar(aes( y = ..prop.., group = 1)) +
    coord_flip() +
    facet_grid(coplayer_type ~ condition, scales = "free") +
    ggtitle(paste0(sp_type, " responders with ", 
                   ifelse(success, "successful", "unsuccessful"), 
                   " training (N=", N, ")"))
}
```

And here is a combined plot

```{r, fig.height = 10, fig.width = 12}
prag_success = get_group_plot("pragmatic", 1)
prag_fail = get_group_plot("pragmatic", 0)
sem_success = get_group_plot("semantic", 1)
sem_fail = get_group_plot("semantic", 0)

plot_grid(
  prag_success,
  prag_fail,
  sem_success,
  sem_fail,
  labels = c("A", "B", "C", "D"), 
  align = "h",
  ncol = 2) %>% show()
```

## Analysis: Proportion of 'win-move' choices

To address our research questions, we look at the data from the main sentence completion task for those participants who completed training successfully. We then record the proportion of "winning moves" for each tuple consisting of the participant, to co-player condition, the sentence type and the participant's classification. If more than 70% of the choices in such a tuple are "winning moves" we consider this a clear majority of win-move choices.

```{r}
# filter and select the relevant bits for analysis
# code the responses as win/lose/opt_out
d_analysis = filter(d, trial_type == "sentence_completion") %>% 
  filter(training_successful == 1) %>% 
  mutate(
    rsp = relevel(factor(
      ifelse(coplayer_type == "unstrategic",
             case_when(response == "red" ~ "win",
                       response == "green" ~ "lose",
                       TRUE ~ "opt_out"),
             case_when(response == "green" ~ "win",
                       response == "red" ~ "lose",
                       TRUE ~ "opt_out"))),
      ref = "win"),
    condition = relevel(factor(condition), ref = "all"),
    item = number,
    win_move = ifelse(rsp == "win", 1, 0)
  ) %>% 
  select(submission_id, item, coplayer_type, condition, tvj_semprag_type, rsp, win_move)


# threshold of minimal proportion of win-moves a player-condition typle
# needs to be counted as "majority-win-move" choice 
winning_threshold = 0.7

# get the "win-move" proportions
d_win_prop = d_analysis %>% 
  group_by(submission_id, coplayer_type, condition, tvj_semprag_type) %>% 
  summarize(win_move_prop = mean(win_move),
            winner = win_move_prop > winning_threshold)
```

We then analyze the distribution of majority-win-choices as a function of co-player type, sentence type, and the participants' semamtic-pragmatic choice types. To so so, we use a Bayesian logistic regression model and inspect the posterior distribution of coefficient values.

```{r}
prior <- c(prior_string("normal(0,10)", class = "b"))
model_win_prop = brm(
  formula = winner ~ coplayer_type * condition * tvj_semprag_type,
  family = "bernoulli",
  data = d_win_prop,
  prior = prior
)

post_samples = posterior_samples(model_win_prop) %>% 
  as_tibble() %>% 
  select( - lp__)
newcolnames = str_replace(names(post_samples), "b_", "") %>% 
  str_replace("condition", "") %>% 
  str_replace("coplayer_type", "") %>% 
  str_replace("tvj_semprag_type", "")
names(post_samples) = newcolnames

bayesplot::mcmc_intervals(post_samples)
```

We see that only few coefficients are estimated to be credibly different from zero. These results suggest the following conclusions:

1. The credibly positive intercept suggests that, in line with visual impression, there is a majority of win-choices overal. Participants who mastered the training also mainly aced the main task.
2. Compared to the cooperative condition (the dummy level), there are credibly less participants who mainly chose the winning move in the competitive conditions (coefs 'strategic' and 'unstrategic'). This is not surprising, given that most language use, if not most of daily interaction, is rather cooperative in nature, so that the competitive condition might just be more difficult.
3. There are credibly less participants who consistently selected the winning move for the 'ad hoc' sentence conditions. This might also be because they are more involved (e.g., involving a slightly different, longer sentence construction).
4. There is *no* credible difference for 'semantic vs pragmatic', suggesting that the answers in the final TVJ did not matter that much: even participants who expected their co-player to judge a literally true sentence with a false implicature as false, relied on implicatures to favor the respective winning move, even in the competitive cases.
5. There is a credibly positive interaction between 'strategic' and 'ad hoc' (not particularly interesting)
6. There is *no* credible difference between 'all' and any other sentence condition (except 'ad hoc') so: 'number' and 'some' had roughly equally high proportions of players who consistently chose the winning move as the semantic conditions (also no further interactions), at this level of analysis.

<!--
## Regression analysis

We see quite clearly that the subgroup of participants who learned from training predominantly use the "normatively correct" option. It is not clear to me (currently) what to test exactly, but here's an analysis nonetheless.

First bring the data into shape, by categorizing responses into "win" (normatively correct), "lose" and "opt_out", based on the condition:

```{r}
d_main = filter(d, trial_type == "sentence_completion") %>% 
  mutate(
    rsp = relevel(factor(
      ifelse(coplayer_type == "unstrategic",
             case_when(response == "red" ~ "win",
                       response == "green" ~ "lose",
                       TRUE ~ "opt_out"),
             case_when(response == "green" ~ "win",
                       response == "red" ~ "lose",
                       TRUE ~ "opt_out"))),
    ref = "win"),
    training = relevel(factor(ifelse(training_successful == 1, "success", "failure")), ref = "success"),
    condition = relevel(factor(condition), ref = "all")
  ) %>% 
  select(rsp, coplayer_type, training, condition, tvj_semprag_type)
```


Then we run a (frequentist) model w/o random effects and w/o interactions (because their intepretation is horrible):

```{r}
## frequentist model w/o random effects 

model_freq = multinom(
  rsp ~ coplayer_type + training + condition + tvj_semprag_type,
  data = d_main
)

# model_freq = step(model_freq)

z = summary(model_freq)$coefficients / summary(model_freq)$standard.errors
p = (1- pnorm(abs(z), 0, 1)) * 2

c_df = t(summary(model_freq)$coefficients) %>% as.data.frame() %>% 
  mutate(coefficient = rownames(t(summary(model_freq)$coefficients))) %>% 
  gather(key = "choice_option", value = "estimate", lose, opt_out)

p_df = t(p) %>% as.data.frame() %>% 
  mutate(coefficient = rownames(t(p))) %>% 
  gather(key = "choice_option", value = "p_value", lose, opt_out)

model_freq_summary = full_join(c_df, p_df, by = c("coefficient", "choice_option")) %>% 
  mutate(
    p_value = signif(p_value, 4),
    signif = case_when(p_value < 0.001 ~ "***",
                            p_value < 0.01 ~ "**",
                            p_value < 0.05 ~ "*",
                            TRUE ~ ""))
show(model_freq_summary)
```

This summary shows the deviation in log-probability of choice options "lose" and "opt_out" from the default category "win". The (dummy coded) default levels of explanatory factors are "cooperative" for co-player type, "success" for training and "all" for sentence condition. We see that "none" and "number" appear not to differ from "all", suggesting that number terms behave more semantically. 

How else should we analyze this? What do we need to test?

-->

